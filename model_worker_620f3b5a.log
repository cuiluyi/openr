2025-02-19 22:11:17 | INFO | stdout | usage: vllm_worker.py [-h] [--host HOST] [--port PORT] [--worker-address WORKER_ADDRESS] [--controller-address CONTROLLER_ADDRESS] [--model-path MODEL_PATH]
2025-02-19 22:11:17 | INFO | stdout |                       [--model-names MODEL_NAMES] [--limit-worker-concurrency LIMIT_WORKER_CONCURRENCY] [--no-register] [--num-gpus NUM_GPUS]
2025-02-19 22:11:17 | INFO | stdout |                       [--conv-template CONV_TEMPLATE] [--trust_remote_code] [--gpu_memory_utilization GPU_MEMORY_UTILIZATION] [--model MODEL]
2025-02-19 22:11:17 | INFO | stdout |                       [--tokenizer TOKENIZER] [--skip-tokenizer-init] [--revision REVISION] [--code-revision CODE_REVISION]
2025-02-19 22:11:17 | INFO | stdout |                       [--tokenizer-revision TOKENIZER_REVISION] [--tokenizer-mode {auto,slow,mistral}] [--trust-remote-code] [--download-dir DOWNLOAD_DIR]
2025-02-19 22:11:17 | INFO | stdout |                       [--load-format {auto,pt,safetensors,npcache,dummy,tensorizer,sharded_state,gguf,bitsandbytes,mistral}] [--config-format {auto,hf,mistral}]
2025-02-19 22:11:17 | INFO | stdout |                       [--dtype {auto,half,float16,bfloat16,float,float32}] [--kv-cache-dtype {auto,fp8,fp8_e5m2,fp8_e4m3}]
2025-02-19 22:11:17 | INFO | stdout |                       [--quantization-param-path QUANTIZATION_PARAM_PATH] [--max-model-len MAX_MODEL_LEN]
2025-02-19 22:11:17 | INFO | stdout |                       [--guided-decoding-backend {outlines,lm-format-enforcer}] [--distributed-executor-backend {ray,mp}] [--worker-use-ray]
2025-02-19 22:11:17 | INFO | stdout |                       [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE] [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
2025-02-19 22:11:17 | INFO | stdout |                       [--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS] [--ray-workers-use-nsight] [--block-size {8,16,32}]
2025-02-19 22:11:17 | INFO | stdout |                       [--enable-prefix-caching] [--disable-sliding-window] [--use-v2-block-manager] [--num-lookahead-slots NUM_LOOKAHEAD_SLOTS] [--seed SEED]
2025-02-19 22:11:17 | INFO | stdout |                       [--swap-space SWAP_SPACE] [--cpu-offload-gb CPU_OFFLOAD_GB] [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
2025-02-19 22:11:17 | INFO | stdout |                       [--num-gpu-blocks-override NUM_GPU_BLOCKS_OVERRIDE] [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS] [--max-num-seqs MAX_NUM_SEQS]
2025-02-19 22:11:17 | INFO | stdout |                       [--max-logprobs MAX_LOGPROBS] [--disable-log-stats]
2025-02-19 22:11:17 | INFO | stdout |                       [--quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,experts_int8,neuron_quant,None}]
2025-02-19 22:11:17 | INFO | stdout |                       [--rope-scaling ROPE_SCALING] [--rope-theta ROPE_THETA] [--enforce-eager] [--max-context-len-to-capture MAX_CONTEXT_LEN_TO_CAPTURE]
2025-02-19 22:11:17 | INFO | stdout |                       [--max-seq-len-to-capture MAX_SEQ_LEN_TO_CAPTURE] [--disable-custom-all-reduce] [--tokenizer-pool-size TOKENIZER_POOL_SIZE]
2025-02-19 22:11:17 | INFO | stdout |                       [--tokenizer-pool-type TOKENIZER_POOL_TYPE] [--tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG]
2025-02-19 22:11:17 | INFO | stdout |                       [--limit-mm-per-prompt LIMIT_MM_PER_PROMPT] [--enable-lora] [--max-loras MAX_LORAS] [--max-lora-rank MAX_LORA_RANK]
2025-02-19 22:11:17 | INFO | stdout |                       [--lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE] [--lora-dtype {auto,float16,bfloat16,float32}]
2025-02-19 22:11:17 | INFO | stdout |                       [--long-lora-scaling-factors LONG_LORA_SCALING_FACTORS] [--max-cpu-loras MAX_CPU_LORAS] [--fully-sharded-loras] [--enable-prompt-adapter]
2025-02-19 22:11:17 | INFO | stdout |                       [--max-prompt-adapters MAX_PROMPT_ADAPTERS] [--max-prompt-adapter-token MAX_PROMPT_ADAPTER_TOKEN]
2025-02-19 22:11:17 | INFO | stdout |                       [--device {auto,cuda,neuron,cpu,openvino,tpu,xpu}] [--num-scheduler-steps NUM_SCHEDULER_STEPS]
2025-02-19 22:11:17 | INFO | stdout |                       [--scheduler-delay-factor SCHEDULER_DELAY_FACTOR] [--enable-chunked-prefill [ENABLE_CHUNKED_PREFILL]]
2025-02-19 22:11:17 | INFO | stdout |                       [--speculative-model SPECULATIVE_MODEL]
2025-02-19 22:11:17 | INFO | stdout |                       [--speculative-model-quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,experts_int8,neuron_quant,None}]
2025-02-19 22:11:17 | INFO | stdout |                       [--num-speculative-tokens NUM_SPECULATIVE_TOKENS] [--speculative-draft-tensor-parallel-size SPECULATIVE_DRAFT_TENSOR_PARALLEL_SIZE]
2025-02-19 22:11:17 | INFO | stdout |                       [--speculative-max-model-len SPECULATIVE_MAX_MODEL_LEN] [--speculative-disable-by-batch-size SPECULATIVE_DISABLE_BY_BATCH_SIZE]
2025-02-19 22:11:17 | INFO | stdout |                       [--ngram-prompt-lookup-max NGRAM_PROMPT_LOOKUP_MAX] [--ngram-prompt-lookup-min NGRAM_PROMPT_LOOKUP_MIN]
2025-02-19 22:11:17 | INFO | stdout |                       [--spec-decoding-acceptance-method {rejection_sampler,typical_acceptance_sampler}]
2025-02-19 22:11:17 | INFO | stdout |                       [--typical-acceptance-sampler-posterior-threshold TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_THRESHOLD]
2025-02-19 22:11:17 | INFO | stdout |                       [--typical-acceptance-sampler-posterior-alpha TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_ALPHA]
2025-02-19 22:11:17 | INFO | stdout |                       [--disable-logprobs-during-spec-decoding [DISABLE_LOGPROBS_DURING_SPEC_DECODING]] [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]
2025-02-19 22:11:17 | INFO | stdout |                       [--ignore-patterns IGNORE_PATTERNS] [--preemption-mode PREEMPTION_MODE] [--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]]
2025-02-19 22:11:17 | INFO | stdout |                       [--qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH] [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]
2025-02-19 22:11:17 | INFO | stdout |                       [--collect-detailed-traces COLLECT_DETAILED_TRACES] [--disable-async-output-proc] [--override-neuron-config OVERRIDE_NEURON_CONFIG]
2025-02-19 22:11:17 | INFO | stdout |                       [--disable-log-requests]
2025-02-19 22:11:17 | INFO | stdout | 
2025-02-19 22:11:17 | INFO | stdout | options:
2025-02-19 22:11:17 | INFO | stdout |   -h, --help            show this help message and exit
2025-02-19 22:11:17 | INFO | stdout |   --host HOST
2025-02-19 22:11:17 | INFO | stdout |   --port PORT
2025-02-19 22:11:17 | INFO | stdout |   --worker-address WORKER_ADDRESS
2025-02-19 22:11:17 | INFO | stdout |   --controller-address CONTROLLER_ADDRESS
2025-02-19 22:11:17 | INFO | stdout |   --model-path MODEL_PATH
2025-02-19 22:11:17 | INFO | stdout |   --model-names MODEL_NAMES
2025-02-19 22:11:17 | INFO | stdout |                         Optional display comma separated names
2025-02-19 22:11:17 | INFO | stdout |   --limit-worker-concurrency LIMIT_WORKER_CONCURRENCY
2025-02-19 22:11:17 | INFO | stdout |   --no-register
2025-02-19 22:11:17 | INFO | stdout |   --num-gpus NUM_GPUS
2025-02-19 22:11:17 | INFO | stdout |   --conv-template CONV_TEMPLATE
2025-02-19 22:11:17 | INFO | stdout |                         Conversation prompt template.
2025-02-19 22:11:17 | INFO | stdout |   --trust_remote_code   Trust remote code (e.g., from HuggingFace) whendownloading the model and tokenizer.
2025-02-19 22:11:17 | INFO | stdout |   --gpu_memory_utilization GPU_MEMORY_UTILIZATION
2025-02-19 22:11:17 | INFO | stdout |                         The ratio (between 0 and 1) of GPU memory toreserve for the model weights, activations, and KV cache. Highervalues will increase the KV
2025-02-19 22:11:17 | INFO | stdout |                         cache size and thus improve the model'sthroughput. However, if the value is too high, it may cause out-of-memory (OOM) errors.
2025-02-19 22:11:17 | INFO | stdout |   --model MODEL         Name or path of the huggingface model to use.
2025-02-19 22:11:17 | INFO | stdout |   --tokenizer TOKENIZER
2025-02-19 22:11:17 | INFO | stdout |                         Name or path of the huggingface tokenizer to use. If unspecified, model name or path will be used.
2025-02-19 22:11:17 | INFO | stdout |   --skip-tokenizer-init
2025-02-19 22:11:17 | INFO | stdout |                         Skip initialization of tokenizer and detokenizer
2025-02-19 22:11:17 | INFO | stdout |   --revision REVISION   The specific model version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.
2025-02-19 22:11:17 | INFO | stdout |   --code-revision CODE_REVISION
2025-02-19 22:11:17 | INFO | stdout |                         The specific revision to use for the model code on Hugging Face Hub. It can be a branch name, a tag name, or a commit id. If
2025-02-19 22:11:17 | INFO | stdout |                         unspecified, will use the default version.
2025-02-19 22:11:17 | INFO | stdout |   --tokenizer-revision TOKENIZER_REVISION
2025-02-19 22:11:17 | INFO | stdout |                         Revision of the huggingface tokenizer to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default
2025-02-19 22:11:17 | INFO | stdout |                         version.
2025-02-19 22:11:17 | INFO | stdout |   --tokenizer-mode {auto,slow,mistral}
2025-02-19 22:11:17 | INFO | stdout |                         The tokenizer mode. * "auto" will use the fast tokenizer if available. * "slow" will always use the slow tokenizer. * "mistral" will
2025-02-19 22:11:17 | INFO | stdout |                         always use the `mistral_common` tokenizer.
2025-02-19 22:11:17 | INFO | stdout |   --trust-remote-code   Trust remote code from huggingface.
2025-02-19 22:11:17 | INFO | stdout |   --download-dir DOWNLOAD_DIR
2025-02-19 22:11:17 | INFO | stdout |                         Directory to download and load the weights, default to the default cache dir of huggingface.
2025-02-19 22:11:17 | INFO | stdout |   --load-format {auto,pt,safetensors,npcache,dummy,tensorizer,sharded_state,gguf,bitsandbytes,mistral}
2025-02-19 22:11:17 | INFO | stdout |                         The format of the model weights to load. * "auto" will try to load the weights in the safetensors format and fall back to the pytorch
2025-02-19 22:11:17 | INFO | stdout |                         bin format if safetensors format is not available. * "pt" will load the weights in the pytorch bin format. * "safetensors" will load the
2025-02-19 22:11:17 | INFO | stdout |                         weights in the safetensors format. * "npcache" will load the weights in pytorch format and store a numpy cache to speed up the loading.
2025-02-19 22:11:17 | INFO | stdout |                         * "dummy" will initialize the weights with random values, which is mainly for profiling. * "tensorizer" will load the weights using
2025-02-19 22:11:17 | INFO | stdout |                         tensorizer from CoreWeave. See the Tensorize vLLM Model script in the Examples section for more information. * "bitsandbytes" will load
2025-02-19 22:11:17 | INFO | stdout |                         the weights using bitsandbytes quantization.
2025-02-19 22:11:17 | INFO | stdout |   --config-format {auto,hf,mistral}
2025-02-19 22:11:17 | INFO | stdout |                         The format of the model config to load. * "auto" will try to load the config in hf format if available else it will try to load in
2025-02-19 22:11:17 | INFO | stdout |                         mistral format
2025-02-19 22:11:17 | INFO | stdout |   --dtype {auto,half,float16,bfloat16,float,float32}
2025-02-19 22:11:17 | INFO | stdout |                         Data type for model weights and activations. * "auto" will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16
2025-02-19 22:11:17 | INFO | stdout |                         models. * "half" for FP16. Recommended for AWQ quantization. * "float16" is the same as "half". * "bfloat16" for a balance between
2025-02-19 22:11:17 | INFO | stdout |                         precision and range. * "float" is shorthand for FP32 precision. * "float32" for FP32 precision.
2025-02-19 22:11:17 | INFO | stdout |   --kv-cache-dtype {auto,fp8,fp8_e5m2,fp8_e4m3}
2025-02-19 22:11:17 | INFO | stdout |                         Data type for kv cache storage. If "auto", will use model data type. CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. ROCm (AMD GPU)
2025-02-19 22:11:17 | INFO | stdout |                         supports fp8 (=fp8_e4m3)
2025-02-19 22:11:17 | INFO | stdout |   --quantization-param-path QUANTIZATION_PARAM_PATH
2025-02-19 22:11:17 | INFO | stdout |                         Path to the JSON file containing the KV cache scaling factors. This should generally be supplied, when KV cache dtype is FP8. Otherwise,
2025-02-19 22:11:17 | INFO | stdout |                         KV cache scaling factors default to 1.0, which may cause accuracy issues. FP8_E5M2 (without scaling) is only supported on cuda
2025-02-19 22:11:17 | INFO | stdout |                         versiongreater than 11.8. On ROCm (AMD GPU), FP8_E4M3 is instead supported for common inference criteria.
2025-02-19 22:11:17 | INFO | stdout |   --max-model-len MAX_MODEL_LEN
2025-02-19 22:11:17 | INFO | stdout |                         Model context length. If unspecified, will be automatically derived from the model config.
2025-02-19 22:11:17 | INFO | stdout |   --guided-decoding-backend {outlines,lm-format-enforcer}
2025-02-19 22:11:17 | INFO | stdout |                         Which engine will be used for guided decoding (JSON schema / regex etc) by default. Currently support https://github.com/outlines-
2025-02-19 22:11:17 | INFO | stdout |                         dev/outlines and https://github.com/noamgat/lm-format-enforcer. Can be overridden per request via guided_decoding_backend parameter.
2025-02-19 22:11:17 | INFO | stdout |   --distributed-executor-backend {ray,mp}
2025-02-19 22:11:17 | INFO | stdout |                         Backend to use for distributed serving. When more than 1 GPU is used, will be automatically set to "ray" if installed or "mp"
2025-02-19 22:11:17 | INFO | stdout |                         (multiprocessing) otherwise.
2025-02-19 22:11:17 | INFO | stdout |   --worker-use-ray      Deprecated, use --distributed-executor-backend=ray.
2025-02-19 22:11:17 | INFO | stdout |   --pipeline-parallel-size PIPELINE_PARALLEL_SIZE, -pp PIPELINE_PARALLEL_SIZE
2025-02-19 22:11:17 | INFO | stdout |                         Number of pipeline stages.
2025-02-19 22:11:17 | INFO | stdout |   --tensor-parallel-size TENSOR_PARALLEL_SIZE, -tp TENSOR_PARALLEL_SIZE
2025-02-19 22:11:17 | INFO | stdout |                         Number of tensor parallel replicas.
2025-02-19 22:11:17 | INFO | stdout |   --max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS
2025-02-19 22:11:17 | INFO | stdout |                         Load model sequentially in multiple batches, to avoid RAM OOM when using tensor parallel and large models.
2025-02-19 22:11:17 | INFO | stdout |   --ray-workers-use-nsight
2025-02-19 22:11:17 | INFO | stdout |                         If specified, use nsight to profile Ray workers.
2025-02-19 22:11:17 | INFO | stdout |   --block-size {8,16,32}
2025-02-19 22:11:17 | INFO | stdout |                         Token block size for contiguous chunks of tokens. This is ignored on neuron devices and set to max-model-len
2025-02-19 22:11:17 | INFO | stdout |   --enable-prefix-caching
2025-02-19 22:11:17 | INFO | stdout |                         Enables automatic prefix caching.
2025-02-19 22:11:17 | INFO | stdout |   --disable-sliding-window
2025-02-19 22:11:17 | INFO | stdout |                         Disables sliding window, capping to sliding window size
2025-02-19 22:11:17 | INFO | stdout |   --use-v2-block-manager
2025-02-19 22:11:17 | INFO | stdout |                         Use BlockSpaceMangerV2.
2025-02-19 22:11:17 | INFO | stdout |   --num-lookahead-slots NUM_LOOKAHEAD_SLOTS
2025-02-19 22:11:17 | INFO | stdout |                         Experimental scheduling config necessary for speculative decoding. This will be replaced by speculative config in the future; it is
2025-02-19 22:11:17 | INFO | stdout |                         present to enable correctness tests until then.
2025-02-19 22:11:17 | INFO | stdout |   --seed SEED           Random seed for operations.
2025-02-19 22:11:17 | INFO | stdout |   --swap-space SWAP_SPACE
2025-02-19 22:11:17 | INFO | stdout |                         CPU swap space size (GiB) per GPU.
2025-02-19 22:11:17 | INFO | stdout |   --cpu-offload-gb CPU_OFFLOAD_GB
2025-02-19 22:11:17 | INFO | stdout |                         The space in GiB to offload to CPU, per GPU. Default is 0, which means no offloading. Intuitively, this argument can be seen as a
2025-02-19 22:11:17 | INFO | stdout |                         virtual way to increase the GPU memory size. For example, if you have one 24 GB GPU and set this to 10, virtually you can think of it as
2025-02-19 22:11:17 | INFO | stdout |                         a 34 GB GPU. Then you can load a 13B model with BF16 weight,which requires at least 26GB GPU memory. Note that this requires fast CPU-
2025-02-19 22:11:17 | INFO | stdout |                         GPU interconnect, as part of the model isloaded from CPU memory to GPU memory on the fly in each model forward pass.
2025-02-19 22:11:17 | INFO | stdout |   --gpu-memory-utilization GPU_MEMORY_UTILIZATION
2025-02-19 22:11:17 | INFO | stdout |                         The fraction of GPU memory to be used for the model executor, which can range from 0 to 1. For example, a value of 0.5 would imply 50%
2025-02-19 22:11:17 | INFO | stdout |                         GPU memory utilization. If unspecified, will use the default value of 0.9.
2025-02-19 22:11:17 | INFO | stdout |   --num-gpu-blocks-override NUM_GPU_BLOCKS_OVERRIDE
2025-02-19 22:11:17 | INFO | stdout |                         If specified, ignore GPU profiling result and use this numberof GPU blocks. Used for testing preemption.
2025-02-19 22:11:17 | INFO | stdout |   --max-num-batched-tokens MAX_NUM_BATCHED_TOKENS
2025-02-19 22:11:17 | INFO | stdout |                         Maximum number of batched tokens per iteration.
2025-02-19 22:11:17 | INFO | stdout |   --max-num-seqs MAX_NUM_SEQS
2025-02-19 22:11:17 | INFO | stdout |                         Maximum number of sequences per iteration.
2025-02-19 22:11:17 | INFO | stdout |   --max-logprobs MAX_LOGPROBS
2025-02-19 22:11:17 | INFO | stdout |                         Max number of log probs to return logprobs is specified in SamplingParams.
2025-02-19 22:11:17 | INFO | stdout |   --disable-log-stats   Disable logging statistics.
2025-02-19 22:11:17 | INFO | stdout |   --quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,experts_int8,neuron_quant,None}, -q {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,experts_int8,neuron_quant,None}
2025-02-19 22:11:17 | INFO | stdout |                         Method used to quantize the weights. If None, we first check the `quantization_config` attribute in the model config file. If that is
2025-02-19 22:11:17 | INFO | stdout |                         None, we assume the model weights are not quantized and use `dtype` to determine the data type of the weights.
2025-02-19 22:11:17 | INFO | stdout |   --rope-scaling ROPE_SCALING
2025-02-19 22:11:17 | INFO | stdout |                         RoPE scaling configuration in JSON format. For example, {"type":"dynamic","factor":2.0}
2025-02-19 22:11:17 | INFO | stdout |   --rope-theta ROPE_THETA
2025-02-19 22:11:17 | INFO | stdout |                         RoPE theta. Use with `rope_scaling`. In some cases, changing the RoPE theta improves the performance of the scaled model.
2025-02-19 22:11:17 | INFO | stdout |   --enforce-eager       Always use eager-mode PyTorch. If False, will use eager mode and CUDA graph in hybrid for maximal performance and flexibility.
2025-02-19 22:11:17 | INFO | stdout |   --max-context-len-to-capture MAX_CONTEXT_LEN_TO_CAPTURE
2025-02-19 22:11:17 | INFO | stdout |                         Maximum context length covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode.
2025-02-19 22:11:17 | INFO | stdout |                         (DEPRECATED. Use --max-seq-len-to-capture instead)
2025-02-19 22:11:17 | INFO | stdout |   --max-seq-len-to-capture MAX_SEQ_LEN_TO_CAPTURE
2025-02-19 22:11:17 | INFO | stdout |                         Maximum sequence length covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode.
2025-02-19 22:11:17 | INFO | stdout |   --disable-custom-all-reduce
2025-02-19 22:11:17 | INFO | stdout |                         See ParallelConfig.
2025-02-19 22:11:17 | INFO | stdout |   --tokenizer-pool-size TOKENIZER_POOL_SIZE
2025-02-19 22:11:17 | INFO | stdout |                         Size of tokenizer pool to use for asynchronous tokenization. If 0, will use synchronous tokenization.
2025-02-19 22:11:17 | INFO | stdout |   --tokenizer-pool-type TOKENIZER_POOL_TYPE
2025-02-19 22:11:17 | INFO | stdout |                         Type of tokenizer pool to use for asynchronous tokenization. Ignored if tokenizer_pool_size is 0.
2025-02-19 22:11:17 | INFO | stdout |   --tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG
2025-02-19 22:11:17 | INFO | stdout |                         Extra config for tokenizer pool. This should be a JSON string that will be parsed into a dictionary. Ignored if tokenizer_pool_size is
2025-02-19 22:11:17 | INFO | stdout |                         0.
2025-02-19 22:11:17 | INFO | stdout |   --limit-mm-per-prompt LIMIT_MM_PER_PROMPT
2025-02-19 22:11:17 | INFO | stdout |                         For each multimodal plugin, limit how many input instances to allow for each prompt. Expects a comma-separated list of items, e.g.:
2025-02-19 22:11:17 | INFO | stdout |                         `image=16,video=2` allows a maximum of 16 images and 2 videos per prompt. Defaults to 1 for each modality.
2025-02-19 22:11:17 | INFO | stdout |   --enable-lora         If True, enable handling of LoRA adapters.
2025-02-19 22:11:17 | INFO | stdout |   --max-loras MAX_LORAS
2025-02-19 22:11:17 | INFO | stdout |                         Max number of LoRAs in a single batch.
2025-02-19 22:11:17 | INFO | stdout |   --max-lora-rank MAX_LORA_RANK
2025-02-19 22:11:17 | INFO | stdout |                         Max LoRA rank.
2025-02-19 22:11:17 | INFO | stdout |   --lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE
2025-02-19 22:11:17 | INFO | stdout |                         Maximum size of extra vocabulary that can be present in a LoRA adapter (added to the base model vocabulary).
2025-02-19 22:11:17 | INFO | stdout |   --lora-dtype {auto,float16,bfloat16,float32}
2025-02-19 22:11:17 | INFO | stdout |                         Data type for LoRA. If auto, will default to base model dtype.
2025-02-19 22:11:17 | INFO | stdout |   --long-lora-scaling-factors LONG_LORA_SCALING_FACTORS
2025-02-19 22:11:17 | INFO | stdout |                         Specify multiple scaling factors (which can be different from base model scaling factor - see eg. Long LoRA) to allow for multiple LoRA
2025-02-19 22:11:17 | INFO | stdout |                         adapters trained with those scaling factors to be used at the same time. If not specified, only adapters trained with the base model
2025-02-19 22:11:17 | INFO | stdout |                         scaling factor are allowed.
2025-02-19 22:11:17 | INFO | stdout |   --max-cpu-loras MAX_CPU_LORAS
2025-02-19 22:11:17 | INFO | stdout |                         Maximum number of LoRAs to store in CPU memory. Must be >= than max_num_seqs. Defaults to max_num_seqs.
2025-02-19 22:11:17 | INFO | stdout |   --fully-sharded-loras
2025-02-19 22:11:17 | INFO | stdout |                         By default, only half of the LoRA computation is sharded with tensor parallelism. Enabling this will use the fully sharded layers. At
2025-02-19 22:11:17 | INFO | stdout |                         high sequence length, max rank or tensor parallel size, this is likely faster.
2025-02-19 22:11:17 | INFO | stdout |   --enable-prompt-adapter
2025-02-19 22:11:17 | INFO | stdout |                         If True, enable handling of PromptAdapters.
2025-02-19 22:11:17 | INFO | stdout |   --max-prompt-adapters MAX_PROMPT_ADAPTERS
2025-02-19 22:11:17 | INFO | stdout |                         Max number of PromptAdapters in a batch.
2025-02-19 22:11:17 | INFO | stdout |   --max-prompt-adapter-token MAX_PROMPT_ADAPTER_TOKEN
2025-02-19 22:11:17 | INFO | stdout |                         Max number of PromptAdapters tokens
2025-02-19 22:11:17 | INFO | stdout |   --device {auto,cuda,neuron,cpu,openvino,tpu,xpu}
2025-02-19 22:11:17 | INFO | stdout |                         Device type for vLLM execution.
2025-02-19 22:11:17 | INFO | stdout |   --num-scheduler-steps NUM_SCHEDULER_STEPS
2025-02-19 22:11:17 | INFO | stdout |                         Maximum number of forward steps per scheduler call.
2025-02-19 22:11:17 | INFO | stdout |   --scheduler-delay-factor SCHEDULER_DELAY_FACTOR
2025-02-19 22:11:17 | INFO | stdout |                         Apply a delay (of delay factor multiplied by previousprompt latency) before scheduling next prompt.
2025-02-19 22:11:17 | INFO | stdout |   --enable-chunked-prefill [ENABLE_CHUNKED_PREFILL]
2025-02-19 22:11:17 | INFO | stdout |                         If set, the prefill requests can be chunked based on the max_num_batched_tokens.
2025-02-19 22:11:17 | INFO | stdout |   --speculative-model SPECULATIVE_MODEL
2025-02-19 22:11:17 | INFO | stdout |                         The name of the draft model to be used in speculative decoding.
2025-02-19 22:11:17 | INFO | stdout |   --speculative-model-quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,experts_int8,neuron_quant,None}
2025-02-19 22:11:17 | INFO | stdout |                         Method used to quantize the weights of speculative model.If None, we first check the `quantization_config` attribute in the model config
2025-02-19 22:11:17 | INFO | stdout |                         file. If that is None, we assume the model weights are not quantized and use `dtype` to determine the data type of the weights.
2025-02-19 22:11:17 | INFO | stdout |   --num-speculative-tokens NUM_SPECULATIVE_TOKENS
2025-02-19 22:11:17 | INFO | stdout |                         The number of speculative tokens to sample from the draft model in speculative decoding.
2025-02-19 22:11:17 | INFO | stdout |   --speculative-draft-tensor-parallel-size SPECULATIVE_DRAFT_TENSOR_PARALLEL_SIZE, -spec-draft-tp SPECULATIVE_DRAFT_TENSOR_PARALLEL_SIZE
2025-02-19 22:11:17 | INFO | stdout |                         Number of tensor parallel replicas for the draft model in speculative decoding.
2025-02-19 22:11:17 | INFO | stdout |   --speculative-max-model-len SPECULATIVE_MAX_MODEL_LEN
2025-02-19 22:11:17 | INFO | stdout |                         The maximum sequence length supported by the draft model. Sequences over this length will skip speculation.
2025-02-19 22:11:17 | INFO | stdout |   --speculative-disable-by-batch-size SPECULATIVE_DISABLE_BY_BATCH_SIZE
2025-02-19 22:11:17 | INFO | stdout |                         Disable speculative decoding for new incoming requests if the number of enqueue requests is larger than this value.
2025-02-19 22:11:17 | INFO | stdout |   --ngram-prompt-lookup-max NGRAM_PROMPT_LOOKUP_MAX
2025-02-19 22:11:17 | INFO | stdout |                         Max size of window for ngram prompt lookup in speculative decoding.
2025-02-19 22:11:17 | INFO | stdout |   --ngram-prompt-lookup-min NGRAM_PROMPT_LOOKUP_MIN
2025-02-19 22:11:17 | INFO | stdout |                         Min size of window for ngram prompt lookup in speculative decoding.
2025-02-19 22:11:17 | INFO | stdout |   --spec-decoding-acceptance-method {rejection_sampler,typical_acceptance_sampler}
2025-02-19 22:11:17 | INFO | stdout |                         Specify the acceptance method to use during draft token verification in speculative decoding. Two types of acceptance routines are
2025-02-19 22:11:17 | INFO | stdout |                         supported: 1) RejectionSampler which does not allow changing the acceptance rate of draft tokens, 2) TypicalAcceptanceSampler which is
2025-02-19 22:11:17 | INFO | stdout |                         configurable, allowing for a higher acceptance rate at the cost of lower quality, and vice versa.
2025-02-19 22:11:17 | INFO | stdout |   --typical-acceptance-sampler-posterior-threshold TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_THRESHOLD
2025-02-19 22:11:17 | INFO | stdout |                         Set the lower bound threshold for the posterior probability of a token to be accepted. This threshold is used by the
2025-02-19 22:11:17 | INFO | stdout |                         TypicalAcceptanceSampler to make sampling decisions during speculative decoding. Defaults to 0.09
2025-02-19 22:11:17 | INFO | stdout |   --typical-acceptance-sampler-posterior-alpha TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_ALPHA
2025-02-19 22:11:17 | INFO | stdout |                         A scaling factor for the entropy-based threshold for token acceptance in the TypicalAcceptanceSampler. Typically defaults to sqrt of
2025-02-19 22:11:17 | INFO | stdout |                         --typical-acceptance-sampler-posterior-threshold i.e. 0.3
2025-02-19 22:11:17 | INFO | stdout |   --disable-logprobs-during-spec-decoding [DISABLE_LOGPROBS_DURING_SPEC_DECODING]
2025-02-19 22:11:17 | INFO | stdout |                         If set to True, token log probabilities are not returned during speculative decoding. If set to False, log probabilities are returned
2025-02-19 22:11:17 | INFO | stdout |                         according to the settings in SamplingParams. If not specified, it defaults to True. Disabling log probabilities during speculative
2025-02-19 22:11:17 | INFO | stdout |                         decoding reduces latency by skipping logprob calculation in proposal sampling, target sampling, and after accepted tokens are
2025-02-19 22:11:17 | INFO | stdout |                         determined.
2025-02-19 22:11:17 | INFO | stdout |   --model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG
2025-02-19 22:11:17 | INFO | stdout |                         Extra config for model loader. This will be passed to the model loader corresponding to the chosen load_format. This should be a JSON
2025-02-19 22:11:17 | INFO | stdout |                         string that will be parsed into a dictionary.
2025-02-19 22:11:17 | INFO | stdout |   --ignore-patterns IGNORE_PATTERNS
2025-02-19 22:11:17 | INFO | stdout |                         The pattern(s) to ignore when loading the model.Default to 'original/**/*' to avoid repeated loading of llama's checkpoints.
2025-02-19 22:11:17 | INFO | stdout |   --preemption-mode PREEMPTION_MODE
2025-02-19 22:11:17 | INFO | stdout |                         If 'recompute', the engine performs preemption by recomputing; If 'swap', the engine performs preemption by block swapping.
2025-02-19 22:11:17 | INFO | stdout |   --served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]
2025-02-19 22:11:17 | INFO | stdout |                         The model name(s) used in the API. If multiple names are provided, the server will respond to any of the provided names. The model name
2025-02-19 22:11:17 | INFO | stdout |                         in the model field of a response will be the first name in this list. If not specified, the model name will be the same as the `--model`
2025-02-19 22:11:17 | INFO | stdout |                         argument. Noted that this name(s)will also be used in `model_name` tag content of prometheus metrics, if multiple names provided,
2025-02-19 22:11:17 | INFO | stdout |                         metricstag will take the first one.
2025-02-19 22:11:17 | INFO | stdout |   --qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH
2025-02-19 22:11:17 | INFO | stdout |                         Name or path of the QLoRA adapter.
2025-02-19 22:11:17 | INFO | stdout |   --otlp-traces-endpoint OTLP_TRACES_ENDPOINT
2025-02-19 22:11:17 | INFO | stdout |                         Target URL to which OpenTelemetry traces will be sent.
2025-02-19 22:11:17 | INFO | stdout |   --collect-detailed-traces COLLECT_DETAILED_TRACES
2025-02-19 22:11:17 | INFO | stdout |                         Valid choices are model,worker,all. It makes sense to set this only if --otlp-traces-endpoint is set. If set, it will collect detailed
2025-02-19 22:11:17 | INFO | stdout |                         traces for the specified modules. This involves use of possibly costly and or blocking operations and hence might have a performance
2025-02-19 22:11:17 | INFO | stdout |                         impact.
2025-02-19 22:11:17 | INFO | stdout |   --disable-async-output-proc
2025-02-19 22:11:17 | INFO | stdout |                         Disable async output processing. This may result in lower performance.
2025-02-19 22:11:17 | INFO | stdout |   --override-neuron-config OVERRIDE_NEURON_CONFIG
2025-02-19 22:11:17 | INFO | stdout |                         override or set neuron device configuration.
2025-02-19 22:11:17 | INFO | stdout |   --disable-log-requests
2025-02-19 22:11:17 | INFO | stdout |                         Disable logging requests.
