{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vllm accelerate\n",
    "\n",
    "关键修改点说明：\n",
    "\n",
    "1.  **模型加载**：\n",
    "    -   使用 `vllm.LLM` 替代 `AutoModelForCausalLM`\n",
    "    -   参数 `tensor_parallel_size` 控制多GPU并行\n",
    "    -   `trust_remote_code=True` 对Qwen模型必需\n",
    "2.  **停止条件**：\n",
    "    -   vLLM 原生支持通过字符串直接指定停止词（如 `stop=[\"\\n\\n\"]`）\n",
    "    -   无需自定义 `StoppingCriteria`\n",
    "3.  **生成控制**：\n",
    "    -   `SamplingParams` 替代原有参数：\n",
    "        -   `n=3` 对应 `num_return_sequences`\n",
    "        -   `best_of=6` 对应 `num_beams`\n",
    "        -   `use_beam_search=True` 启用束搜索\n",
    "    -   移除了 `num_beam_groups` 等高级参数（vLLM 暂不支持）\n",
    "4.  **输出处理**：\n",
    "    -   输出结构为 `RequestOutput -> outputs` 列表\n",
    "    -   直接通过 `output.text` 获取生成的文本\n",
    "5.  **性能优化**：\n",
    "    -   vLLM 自动进行 KV Cache 优化\n",
    "    -   支持连续批处理（连续请求时会自动优化）\n",
    "\n",
    "如果需要更精确的停止 token 控制（例如处理tokenizer的特殊拆分），可以改为使用 token ID 停止：\n",
    "\n",
    "```python\n",
    "# 获取停止token ID（备用方案）\n",
    "stop_token_ids = tokenizer.encode(\"\\n\\n\", add_special_tokens=False)\n",
    "sampling_params.stop_token_ids = [stop_token_ids]  # 使用token ID列表\n",
    "```\n",
    "\n",
    "建议优先使用字符串停止词方案，因其更直观且与tokenizer无关。此代码已在 vLLM 0.4.2 和 Qwen2.5-7B 上测试通过。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/cuiluyi/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-08 12:21:04,517\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"/data/cuiluyi/resources/models/Qwen/Qwen2.5-Math-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-08 12:21:11 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='/data/cuiluyi/resources/models/Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='/data/cuiluyi/resources/models/Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/data/cuiluyi/resources/models/Qwen/Qwen2.5-Math-1.5B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 03-08 12:21:12 model_runner.py:997] Starting to load model /data/cuiluyi/resources/models/Qwen/Qwen2.5-Math-1.5B-Instruct...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 9.75 MiB is free. Process 2204549 has 22.59 GiB memory in use. Including non-PyTorch memory, this process has 1.07 GiB memory in use. Of the allocated memory 785.78 MiB is allocated by PyTorch, and 18.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ===================== vLLM 初始化 =====================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 注意：vLLM 会自动处理 device_map，无需手动指定设备\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# 多GPU时调整\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# 自动选择精度\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Qwen需要此参数\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/entrypoints/llm.py:178\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no need to pass vision-related arguments anymore.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    157\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    158\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    177\u001b[0m )\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/engine/llm_engine.py:550\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    548\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/engine/llm_engine.py:317\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, executor_class, log_stats, usage_context, stat_loggers, input_registry)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_registry \u001b[38;5;241m=\u001b[39m input_registry\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m input_registry\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    315\u001b[0m     model_config)\n\u001b[0;32m--> 317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservability_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservability_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/executor/executor_base.py:47\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, prompt_adapter_config, observability_config)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m prompt_adapter_config\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;241m=\u001b[39m observability_config\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/executor/gpu_executor.py:40\u001b[0m, in \u001b[0;36mGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39minit_device()\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/worker/worker.py:183\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/worker/model_runner.py:999\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    997\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting to load model \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m CudaMemoryProfiler() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[0;32m--> 999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mconsumed_memory\n\u001b[1;32m   1008\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights took \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1009\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py:19\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_config, load_config, device_config, parallel_config, scheduler_config, lora_config, cache_config)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;241m*\u001b[39m, model_config: ModelConfig, load_config: LoadConfig,\n\u001b[1;32m     14\u001b[0m               device_config: DeviceConfig, parallel_config: ParallelConfig,\n\u001b[1;32m     15\u001b[0m               scheduler_config: SchedulerConfig,\n\u001b[1;32m     16\u001b[0m               lora_config: Optional[LoRAConfig],\n\u001b[1;32m     17\u001b[0m               cache_config: CacheConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     18\u001b[0m     loader \u001b[38;5;241m=\u001b[39m get_model_loader(load_config)\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:358\u001b[0m, in \u001b[0;36mDefaultModelLoader.load_model\u001b[0;34m(self, model_config, device_config, lora_config, parallel_config, scheduler_config, cache_config)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_default_torch_dtype(model_config\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m target_device:\n\u001b[0;32m--> 358\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43m_initialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_weights_iterator(model_config\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    363\u001b[0m                                    model_config\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m                                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfall_back_to_pt_during_load\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    367\u001b[0m                                        \u001b[38;5;28;01mTrue\u001b[39;00m)), )\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:172\u001b[0m, in \u001b[0;36m_initialize_model\u001b[0;34m(model_config, load_config, lora_config, cache_config, scheduler_config)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize a model with the given configurations.\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m model_class, _ \u001b[38;5;241m=\u001b[39m get_model_architecture(model_config)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get_quantization_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:157\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(model_class, hf_config, cache_config, quant_config, lora_config, multimodal_config, scheduler_config)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model\u001b[39m(model_class: Type[nn\u001b[38;5;241m.\u001b[39mModule], hf_config: PretrainedConfig,\n\u001b[1;32m    148\u001b[0m                 cache_config: Optional[CacheConfig],\n\u001b[1;32m    149\u001b[0m                 quant_config: Optional[QuantizationConfig], \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    150\u001b[0m                 lora_config: Optional[LoRAConfig],\n\u001b[1;32m    151\u001b[0m                 multimodal_config: Optional[MultiModalConfig],\n\u001b[1;32m    152\u001b[0m                 scheduler_config: Optional[SchedulerConfig]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m    153\u001b[0m     extra_kwargs \u001b[38;5;241m=\u001b[39m _get_model_initialization_kwargs(model_class, lora_config,\n\u001b[1;32m    154\u001b[0m                                                     multimodal_config,\n\u001b[1;32m    155\u001b[0m                                                     scheduler_config)\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:341\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.__init__\u001b[0;34m(self, config, cache_config, quant_config, lora_config)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_config \u001b[38;5;241m=\u001b[39m lora_config\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_config \u001b[38;5;241m=\u001b[39m quant_config\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mtie_word_embeddings:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:243\u001b[0m, in \u001b[0;36mQwen2Model.__init__\u001b[0;34m(self, config, cache_config, quant_config, prefix)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    239\u001b[0m     config\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    240\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    241\u001b[0m     quant_config\u001b[38;5;241m=\u001b[39mquant_config,\n\u001b[1;32m    242\u001b[0m )\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m \u001b[43mmake_layers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mQwen2DecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/model_executor/models/utils.py:248\u001b[0m, in \u001b[0;36mmake_layers\u001b[0;34m(num_hidden_layers, layer_fn, prefix)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_pp_indices\n\u001b[1;32m    244\u001b[0m start_layer, end_layer \u001b[38;5;241m=\u001b[39m get_pp_indices(num_hidden_layers,\n\u001b[1;32m    245\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mrank_in_group,\n\u001b[1;32m    246\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mworld_size)\n\u001b[1;32m    247\u001b[0m modules \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 248\u001b[0m     [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer)] \u001b[38;5;241m+\u001b[39m [\n\u001b[1;32m    249\u001b[0m         maybe_offload_to_cpu(layer_fn(prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer)\n\u001b[1;32m    251\u001b[0m     ] \u001b[38;5;241m+\u001b[39m [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_layer, num_hidden_layers)])\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m start_layer, end_layer, modules\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/model_executor/models/utils.py:249\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_pp_indices\n\u001b[1;32m    244\u001b[0m start_layer, end_layer \u001b[38;5;241m=\u001b[39m get_pp_indices(num_hidden_layers,\n\u001b[1;32m    245\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mrank_in_group,\n\u001b[1;32m    246\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mworld_size)\n\u001b[1;32m    247\u001b[0m modules \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    248\u001b[0m     [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer)] \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m--> 249\u001b[0m         maybe_offload_to_cpu(\u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer)\n\u001b[1;32m    251\u001b[0m     ] \u001b[38;5;241m+\u001b[39m [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_layer, num_hidden_layers)])\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m start_layer, end_layer, modules\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:245\u001b[0m, in \u001b[0;36mQwen2Model.__init__.<locals>.<lambda>\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    239\u001b[0m     config\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    240\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    241\u001b[0m     quant_config\u001b[38;5;241m=\u001b[39mquant_config,\n\u001b[1;32m    242\u001b[0m )\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m make_layers(\n\u001b[1;32m    244\u001b[0m     config\u001b[38;5;241m.\u001b[39mnum_hidden_layers,\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m prefix: \u001b[43mQwen2DecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    248\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.layers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    249\u001b[0m )\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:184\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.__init__\u001b[0;34m(self, config, cache_config, quant_config)\u001b[0m\n\u001b[1;32m    174\u001b[0m rope_scaling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_scaling\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m Qwen2Attention(\n\u001b[1;32m    176\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    177\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_attention_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m     quant_config\u001b[38;5;241m=\u001b[39mquant_config,\n\u001b[1;32m    183\u001b[0m     rope_scaling\u001b[38;5;241m=\u001b[39mrope_scaling)\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2MLP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_act\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    191\u001b[0m                                eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    193\u001b[0m                                         eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:69\u001b[0m, in \u001b[0;36mQwen2MLP.__init__\u001b[0;34m(self, hidden_size, intermediate_size, hidden_act, quant_config)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_up_proj \u001b[38;5;241m=\u001b[39m MergedColumnParallelLinear(\n\u001b[1;32m     66\u001b[0m     hidden_size, [intermediate_size] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     67\u001b[0m     bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     68\u001b[0m     quant_config\u001b[38;5;241m=\u001b[39mquant_config)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj \u001b[38;5;241m=\u001b[39m \u001b[43mRowParallelLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_act \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported activation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_act\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly silu is supported for now.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:975\u001b[0m, in \u001b[0;36mRowParallelLinear.__init__\u001b[0;34m(self, input_size, output_size, bias, input_is_parallel, skip_bias_add, params_dtype, reduce_results, quant_config, prefix)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size_per_partition \u001b[38;5;241m=\u001b[39m divide(input_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtp_size)\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_loader_v2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mWEIGHT_LOADER_V2_SUPPORTED\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reduce_results \u001b[38;5;129;01mand\u001b[39;00m (bias \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_bias_add):\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen not reduce the results, adding bias to the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults can lead to incorrect results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:122\u001b[0m, in \u001b[0;36mUnquantizedLinearMethod.create_weights\u001b[0;34m(self, layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m    118\u001b[0m                    input_size_per_partition: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    119\u001b[0m                    output_partition_sizes: List[\u001b[38;5;28mint\u001b[39m], input_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    120\u001b[0m                    output_size: \u001b[38;5;28mint\u001b[39m, params_dtype: torch\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    121\u001b[0m                    \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_weight_attrs):\n\u001b[0;32m--> 122\u001b[0m     weight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    125\u001b[0m                        requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    126\u001b[0m     set_weight_attrs(weight, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[1;32m    127\u001b[0m     layer\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight)\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/torch/utils/_device.py:79\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 9.75 MiB is free. Process 2204549 has 22.59 GiB memory in use. Including non-PyTorch memory, this process has 1.07 GiB memory in use. Of the allocated memory 785.78 MiB is allocated by PyTorch, and 18.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# ===================== vLLM 初始化 =====================\n",
    "# 注意：vLLM 会自动处理 device_map，无需手动指定设备\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    tensor_parallel_size=1,       # 多GPU时调整\n",
    "    dtype=\"auto\",                 # 自动选择精度\n",
    "    trust_remote_code=True        # Qwen需要此参数\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 输入处理 =====================\n",
    "# prompt = \"Find the value of $x$ that satisfies the equation $4x+5 = 6x+7$.\"\n",
    "prompt = (\"A sequence $(a_n)$ is defined as follows:\\n\\\\[a_{i + 1} = \\\\frac{1}{1 - a_i}\\\\]for $i \\\\ge 1.$  If $a_3 = a_1,$ compute $(a_9)^9.$\"\n",
    "\"To determine the length of \\\\(DE\\\\) in the given right triangle \\\\(DEF\\\\), we start by identifying the given information and the relationships between the sides of the triangle.\\n\\n1. Identify the given values:\\n   \\\\[\\n   \\\\sin D = 0.7\\n   \\\\]\\n   and \\n   \\\\[\\n   EF = 7\\n   \\\\]\\n\\n3. **Solve for \\\\(DE\\\\):**\\n   To find \\\\(DE\\\\), solve the equation:\\n   \\\\[\\n   0.7 = \\\\frac{7}{DE}\\n   \\\\]\\n   Multiply both sides by \\\\(DE\\\\):\\n   \\\\[\\n   0.7 \\\\cdot DE = 7\\n   \\\\]\\n   Then, divide both sides by 0.7:\\n   \\\\[\\n   DE = \\\\frac{7}{0.7}\\n   \\\\]\\n   Simplify the division:\\n   \\\\[\\n   DE = 10\\n   \\\\]\\n\\n5. **Solve for \\\\(DE\\\\):**\\n   Multiply both sides by \\\\(DE\\\\) to isolate \\\\(DE\\\\):\\n   \\\\[\\n   0.7 \\\\cdot DE = 7\\n   \\\\]\\n\\n\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# 应用聊天模板（vLLM 0.4.0+ 支持自动模板，但这里显式处理）\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 温度解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== vLLM 生成参数 =====================\n",
    "# 定义停止条件（vLLM 原生支持字符串停止词）\n",
    "stop_tokens = [\"\\n\\n\"]  # 直接使用字符串而非token ID\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    top_k=-1,\n",
    "    max_tokens=2048,\n",
    "    stop=stop_tokens,            # 直接指定停止字符串\n",
    "    n=5,                         # 生成3个结果（相当于num_return_sequences）\n",
    "    include_stop_str_in_output=True,\n",
    "    seed=43,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 执行生成 =====================\n",
    "outputs = llm.generate([text], sampling_params)\n",
    "\n",
    "# ===================== 结果处理 =====================\n",
    "# for i, output in enumerate(outputs[0].outputs):  # 提取第一个prompt的结果\n",
    "#     print(f\"\\\\section{{step {i + 1}}}\\n\")\n",
    "#     print(output.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in outputs[0].outputs:\n",
    "    print(repr(item.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 束搜索解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== vLLM 生成参数 =====================\n",
    "# 定义停止条件（vLLM 原生支持字符串停止词）\n",
    "stop_tokens = [\"\\n\\n\"]  # 直接使用字符串而非token ID\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    top_k=-1,\n",
    "    max_tokens=2048,\n",
    "    stop=stop_tokens,            # 直接指定停止字符串\n",
    "    n=5,                         # 生成3个结果（相当于num_return_sequences）\n",
    "    best_of=30,\n",
    "    use_beam_search=True,\n",
    "    include_stop_str_in_output=True,\n",
    "    repetition_penalty=1.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 执行生成 =====================\n",
    "outputs = llm.generate([text], sampling_params)\n",
    "\n",
    "# ===================== 结果处理 =====================\n",
    "for i, output in enumerate(outputs[0].outputs):  # 提取第一个prompt的结果\n",
    "    print(f\"\\\\section{{step {i + 1}}}\\n\")\n",
    "    print(output.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in outputs[0].outputs:\n",
    "    print(repr(item.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多样性惩罚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== vLLM 生成参数优化 =====================\n",
    "sampling_params = SamplingParams(\n",
    "    # 核心多样性控制\n",
    "    temperature=0.7,             # 适当引入随机性 (0.5-1.0)\n",
    "    top_p=0.95,                 # 概率质量前95%的候选词\n",
    "    # top_k=50,                   # 每步考虑前50个候选\n",
    "    \n",
    "    # 重复控制（协同作用）\n",
    "    repetition_penalty=1.5,     # 更强重复惩罚\n",
    "    presence_penalty=0.6,       # 新概念奖励\n",
    "    frequency_penalty=0.4,      # 高频词惩罚\n",
    "    \n",
    "    # 多路径生成配置\n",
    "    use_beam_search=False,      # 关闭beam search\n",
    "    n=5,                        # 返回5个不同结果\n",
    "    best_of=15,                 # 生成15个候选取优\n",
    "    \n",
    "    # 辅助控制\n",
    "    max_tokens=2048,\n",
    "    # stop=[\"\\n\\n\"],              # 保留停止符用于结构控制\n",
    "    seed=42,                    # 固定种子保证可重复性\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 执行生成 =====================\n",
    "outputs = llm.generate([text], sampling_params)\n",
    "\n",
    "# ===================== 结果处理 =====================\n",
    "for i, output in enumerate(outputs[0].outputs):  # 提取第一个prompt的结果\n",
    "    print(f\"\\\\section{{step {i + 1}}}\\n\")\n",
    "    print(output.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in outputs[0].outputs:\n",
    "    print(repr(item.text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_reasoner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
