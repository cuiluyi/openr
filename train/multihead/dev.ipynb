{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "device = \"cuda\" # the device to load the model onto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/cuiluyi/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Config, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "\n",
    "# ============== 1. 修正模型类定义 ==============\n",
    "class MultiHeadQwen2(Qwen2ForCausalLM):\n",
    "    def __init__(self, config, num_heads=3):\n",
    "        # 先初始化原始模型结构\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # 删除原单一头并创建多头\n",
    "        del self.lm_head\n",
    "        self.num_heads = num_heads\n",
    "        self.lm_heads = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        # 注册自定义参数\n",
    "        self.current_head = 0\n",
    "        self.head_switch_freq = 0\n",
    "\n",
    "    def prepare_inputs_for_generation(self, *args, **kwargs):\n",
    "        # 从generation_config获取参数\n",
    "        gen_config = kwargs.get(\"generation_config\", self.generation_config)\n",
    "        self.current_head = getattr(gen_config, \"head_idx\", 0)\n",
    "        self.head_switch_freq = getattr(gen_config, \"head_switch_freq\", 0)\n",
    "        return super().prepare_inputs_for_generation(*args, **kwargs)\n",
    "\n",
    "    def forward(self, input_ids=None, **kwargs):\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            output_hidden_states=True,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # 动态切换逻辑\n",
    "        if self.head_switch_freq > 0 and not self.training:\n",
    "            seq_len = input_ids.shape[-1]\n",
    "            if seq_len % self.head_switch_freq == 0:\n",
    "                self.current_head = (self.current_head + 1) % self.num_heads\n",
    "        \n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        logits = self.lm_heads[self.current_head](hidden_states)\n",
    "        return (logits,) + outputs[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 2. 修正模型加载方法 ==============\n",
    "def load_model_with_heads(checkpoint, num_heads=3):\n",
    "    # 单独加载配置\n",
    "    config = Qwen2Config.from_pretrained(checkpoint)\n",
    "    \n",
    "    # 手动初始化模型\n",
    "    model = MultiHeadQwen2(config, num_heads=num_heads)\n",
    "    \n",
    "    # 加载预训练权重（跳过不匹配的 lm_head）\n",
    "    pretrained = Qwen2ForCausalLM.from_pretrained(checkpoint)\n",
    "    model.load_state_dict(pretrained.state_dict(), strict=False)\n",
    "    \n",
    "    # 复制原始head到所有新头\n",
    "    original_head = pretrained.lm_head.state_dict()\n",
    "    for head in model.lm_heads:\n",
    "        head.load_state_dict(original_head)\n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 3. 使用示例 ==============\n",
    "checkpoint = \"/data/cuiluyi/resources/models/Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = load_model_with_heads(checkpoint, num_heads=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Find the value of $x$ that satisfies the equation $4x+5 = 6x+7$.\"\n",
    "\n",
    "# CoT\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = Qwen2ForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['attention_mask'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 示例1：固定头生成\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# generation_config=GenerationConfig(\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     max_new_tokens=512,\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     do_sample=True,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     temperature=0.7,\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     # 自定义参数\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     head_idx=1  # 固定使用第二个头\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# )\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     output_ids[\u001b[38;5;28mlen\u001b[39m(input_ids):] \u001b[38;5;28;01mfor\u001b[39;00m input_ids, output_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_inputs\u001b[38;5;241m.\u001b[39minput_ids, generated_ids)\n\u001b[1;32m     15\u001b[0m ]\n\u001b[1;32m     17\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/transformers/generation/utils.py:1804\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1802\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(generation_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1804\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model)\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/open_reasoner/lib/python3.10/site-packages/transformers/generation/utils.py:1205\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1208\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['attention_mask'] (note: typos in the generate arguments will also show up in this list)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 示例1：固定头生成\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    # generation_config=GenerationConfig(\n",
    "    #     max_new_tokens=512,\n",
    "    #     do_sample=True,\n",
    "    #     temperature=0.7,\n",
    "    #     # 自定义参数\n",
    "    #     head_idx=1  # 固定使用第二个头\n",
    "    # )\n",
    ")\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例2：动态切换头（每3个token切换一次）\n",
    "output_dynamic = model.generate(\n",
    "    tokenizer(\"Solve 5x-3=12:\", return_tensors=\"pt\").to(device),\n",
    "    generation_config=GenerationConfig(\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        # 自定义参数\n",
    "        head_switch_freq=3  # 切换频率\n",
    "    )\n",
    ")\n",
    "print(tokenizer.decode(output_dynamic[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_reasoner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
